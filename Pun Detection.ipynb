{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RwEwboy6QAv",
        "outputId": "7a70dfa0-c87a-435c-ad8b-fc5c3bc97c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pronouncing in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pronouncing) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.12/dist-packages (from cmudict>=0.4.0->pronouncing) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.12/dist-packages (from cmudict>=0.4.0->pronouncing) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=5->cmudict>=0.4.0->pronouncing) (3.23.0)\n",
            "Requirement already satisfied: phonetics in /usr/local/lib/python3.12/dist-packages (1.0.5)\n"
          ]
        }
      ],
      "source": [
        "#!pip install pronouncing\n",
        "#!pip install phonetics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qza5GhNtz02"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDIYtI90iXsQDsIOytwdrFBeaIeVEwjnTs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlKdLZb9OwvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d91767-182a-4481-ff47-ef2593abc580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… AoA entries loaded: 31,104\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_aoa(path=\"/content/drive/MyDrive/Colab Notebooks/AoA_ratings_Kuperman_et_al_BRM_with_PoS.xlsx\"):\n",
        "    try:\n",
        "        df = pd.read_excel(path)\n",
        "        w = next((c for c in df.columns if \"word\" in c.lower()), \"Word\")\n",
        "        r = next((c for c in df.columns if any(k in c.lower() for k in [\"aoa\",\"mean\",\"rating\"])), \"Rating.Mean\")\n",
        "        df = df[[w,r]].dropna()\n",
        "        df[w] = df[w].astype(str).str.lower()\n",
        "        print(f\"âœ… AoA entries loaded: {len(df):,}\")\n",
        "        return dict(zip(df[w], df[r]))\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ AoA file not found or error loading: {e} â€” using fallback.\")\n",
        "        return {} # Return an empty dictionary or None as a fallback\n",
        "\n",
        "AOA = load_aoa()\n",
        "\n",
        "\n",
        "def aoa_penalty(word, age):\n",
        "    val = AOA.get(word.lower())\n",
        "    if val is None:\n",
        "        print(f\"Word '{word}' has not been found in AoA Dict.\")\n",
        "        return None # Or return a default penalty or value\n",
        "    try:\n",
        "        age = float(age)\n",
        "        return print(f'{age > val} : The word is suitable for age greater than:{val:.2f}')\n",
        "    except ValueError:\n",
        "        print(f\"Invalid age provided: {age}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaBy6xH3PGZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7196c60e-26ee-41fa-efd3-37695e90de3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_x0JcE0wPnR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmpx2pUFMFw6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVdy3mFl7H7X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "outputId": "daf0835b-4997-4873-a2bb-3d9e0128dd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Checking available models...\n",
            "  âœ“ models/gemini-2.5-flash\n",
            "  âœ“ models/gemini-2.5-pro\n",
            "  âœ“ models/gemini-2.0-flash-exp\n",
            "  âœ“ models/gemini-2.0-flash\n",
            "  âœ“ models/gemini-2.0-flash-001\n",
            "  âœ“ models/gemini-2.0-flash-exp-image-generation\n",
            "  âœ“ models/gemini-2.0-flash-lite-001\n",
            "  âœ“ models/gemini-2.0-flash-lite\n",
            "  âœ“ models/gemini-2.0-flash-lite-preview-02-05\n",
            "  âœ“ models/gemini-2.0-flash-lite-preview\n",
            "  âœ“ models/gemini-exp-1206\n",
            "  âœ“ models/gemini-2.5-flash-preview-tts\n",
            "  âœ“ models/gemini-2.5-pro-preview-tts\n",
            "  âœ“ models/gemma-3-1b-it\n",
            "  âœ“ models/gemma-3-4b-it\n",
            "  âœ“ models/gemma-3-12b-it\n",
            "  âœ“ models/gemma-3-27b-it\n",
            "  âœ“ models/gemma-3n-e4b-it\n",
            "  âœ“ models/gemma-3n-e2b-it\n",
            "  âœ“ models/gemini-flash-latest\n",
            "  âœ“ models/gemini-flash-lite-latest\n",
            "  âœ“ models/gemini-pro-latest\n",
            "  âœ“ models/gemini-2.5-flash-lite\n",
            "  âœ“ models/gemini-2.5-flash-image-preview\n",
            "  âœ“ models/gemini-2.5-flash-image\n",
            "  âœ“ models/gemini-2.5-flash-preview-09-2025\n",
            "  âœ“ models/gemini-2.5-flash-lite-preview-09-2025\n",
            "  âœ“ models/gemini-3-pro-preview\n",
            "  âœ“ models/gemini-3-pro-image-preview\n",
            "  âœ“ models/nano-banana-pro-preview\n",
            "  âœ“ models/gemini-robotics-er-1.5-preview\n",
            "  âœ“ models/gemini-2.5-computer-use-preview-10-2025\n",
            "\n",
            "âœ… Using model: gemini-pro-latest\n",
            "\n",
            "Loading BERT and CMU dictionary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Hybrid Pun/Joke Detector â€” Core Cell (UPDATED: implicit search skips same-lemma words)\n",
        "# Drop-in cell: safe to paste/replace your existing cell.\n",
        "# Changes:\n",
        "#   â€¢ Added `_lemma_set()` and lemma-based filtering in `_best_implicit_phonetic_partner(...)`\n",
        "#     so implicit phonetic matches like \"water\"â†”\"watery\" are skipped; will consider \"what-are\" etc.\n",
        "#   â€¢ `extract_features(...)` calls implicit search with `avoid_same_lemma=True`.\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import os, json, re, time, torch, numpy as np, pandas as pd\n",
        "import google.generativeai as genai\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import cmudict, stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from scipy.spatial.distance import cosine\n",
        "import phonetics\n",
        "from collections import defaultdict\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('cmudict', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)                 # [kept]\n",
        "nltk.download('wordnet', quiet=True)                   # [kept]\n",
        "nltk.download('omw-1.4', quiet=True)                  # [kept]\n",
        "\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"Please set GOOGLE_API_KEY environment variable (export GOOGLE_API_KEY='your-key')\")\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "print(\"ðŸ” Checking available models...\")\n",
        "available_models = []\n",
        "for m in genai.list_models():\n",
        "    if 'generateContent' in getattr(m, \"supported_generation_methods\", []):\n",
        "        name = getattr(m, \"name\", \"\")\n",
        "        print(f\"  âœ“ {name}\")\n",
        "        available_models.append(name)\n",
        "\n",
        "if not available_models:\n",
        "    raise ValueError(\"No generative models are available. Check your API key/permissions.\")\n",
        "\n",
        "preferred = None\n",
        "for nm in available_models:\n",
        "    low = nm.lower()\n",
        "    if \"gemini-1.5-pro\" in low:\n",
        "        preferred = nm.split(\"/\")[-1]; break\n",
        "if not preferred:\n",
        "    for nm in available_models:\n",
        "        low = nm.lower()\n",
        "        if \"gemini-pro\" in low:\n",
        "            preferred = nm.split(\"/\")[-1]; break\n",
        "if not preferred:\n",
        "    preferred = available_models[0].split(\"/\")[-1]\n",
        "\n",
        "print(f\"\\nâœ… Using model: {preferred}\\n\")\n",
        "model_llm = genai.GenerativeModel(preferred)\n",
        "\n",
        "print(\"Loading BERT and CMU dictionary...\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert.eval()\n",
        "cmu = cmudict.dict()\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "WNL = WordNetLemmatizer()\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "STOP.update({\"n't\", \"not\"})\n",
        "\n",
        "AUXILIARIES = {\n",
        "    \"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\"having\",\n",
        "    \"do\",\"does\",\"did\",\"doing\",\n",
        "    \"can\",\"could\",\"may\",\"might\",\"must\",\"shall\",\"should\",\"will\",\"would\",\n",
        "    \"ain\",\"ain't\",\"â€™m\",\"'m\",\"â€™re\",\"'re\",\"â€™ve\",\"'ve\",\"â€™ll\",\"'ll\",\"â€™d\",\"'d\"\n",
        "}\n",
        "\n",
        "CONTRACTION_PATTERNS = [\n",
        "    (re.compile(r\"\\b(\\w+)[â€™']ll\\b\", flags=re.IGNORECASE), r\"\\1 will\"),\n",
        "    (re.compile(r\"\\b(\\w+)[â€™']re\\b\", flags=re.IGNORECASE), r\"\\1 are\"),\n",
        "    (re.compile(r\"\\b(\\w+)[â€™']ve\\b\", flags=re.IGNORECASE), r\"\\1 have\"),\n",
        "    (re.compile(r\"\\b(\\w+)[â€™']d\\b\", flags=re.IGNORECASE), r\"\\1 would\"),\n",
        "    (re.compile(r\"\\b(\\w+)[â€™']m\\b\", flags=re.IGNORECASE), r\"\\1 am\"),\n",
        "    (re.compile(r\"\\b(\\w+)n[â€™']t\\b\", flags=re.IGNORECASE), r\"\\1 not\"),\n",
        "    (re.compile(r\"[â€™']\", flags=re.IGNORECASE), r\"'\"),\n",
        "]\n",
        "\n",
        "def expand_contractions(text: str) -> str:\n",
        "    t = text\n",
        "    for pat, repl in CONTRACTION_PATTERNS:\n",
        "        t = pat.sub(repl, t)\n",
        "    return t\n",
        "\n",
        "def penn_to_wn(pos_tag_ch):\n",
        "    \"\"\"Map Penn Treebank tag initial to WordNet POS\"\"\"\n",
        "    if pos_tag_ch.startswith('J'): return wordnet.ADJ\n",
        "    if pos_tag_ch.startswith('V'): return wordnet.VERB\n",
        "    if pos_tag_ch.startswith('N'): return wordnet.NOUN\n",
        "    if pos_tag_ch.startswith('R'): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def lemma_token(token, pos_tag_ch='N'):\n",
        "    return WNL.lemmatize(token, pos=penn_to_wn(pos_tag_ch))\n",
        "\n",
        "# [NEW] Build a set of lemmas across POS to detect \"same-base word\" relations.\n",
        "def _lemma_set(word: str):\n",
        "    w = (word or \"\").lower()\n",
        "    if not w:\n",
        "        return set()\n",
        "    poss = [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]\n",
        "    return { WNL.lemmatize(w, pos=p) for p in poss } | { WNL.lemmatize(w) }\n",
        "\n",
        "def _levenshtein(a, b):\n",
        "    if isinstance(a, str): a = list(a)\n",
        "    if isinstance(b, str): b = list(b)\n",
        "    n, m = len(a), len(b)\n",
        "    if n == 0: return m\n",
        "    if m == 0: return n\n",
        "    dp = list(range(m + 1))\n",
        "    for i in range(1, n + 1):\n",
        "        prev = dp[0]\n",
        "        dp[0] = i\n",
        "        ai = a[i - 1]\n",
        "        for j in range(1, m + 1):\n",
        "            cur = dp[j]\n",
        "            cost = 0 if ai == b[j - 1] else 1\n",
        "            dp[j] = min(dp[j] + 1, dp[j - 1] + 1, prev + cost)\n",
        "            prev = cur\n",
        "    return dp[m]\n",
        "\n",
        "def _edge_match_bonus(seq1, seq2, first_bonus=0.05, last_bonus=0.07):\n",
        "    \"\"\"Small bonus when first/last symbols align (phones or chars).\"\"\"\n",
        "    if not seq1 or not seq2:\n",
        "        return 0.0\n",
        "    bonus = 0.0\n",
        "    if seq1[0] == seq2[0]:\n",
        "        bonus += first_bonus\n",
        "    if seq1[-1] == seq2[-1]:\n",
        "        bonus += last_bonus\n",
        "    return bonus\n",
        "\n",
        "def _weighted_sim_from_distance(dist, len_a, len_b, beta=2.0):\n",
        "    denom = max(len_a, len_b, 1)\n",
        "    d = dist / denom\n",
        "    s = 1.0 - (d ** beta)\n",
        "    return float(max(0.0, min(1.0, s)))\n",
        "\n",
        "def _letter_penalty(w1, w2):\n",
        "    d = _levenshtein(w1.lower(), w2.lower())\n",
        "    return d / max(len(w1), len(w2), 1)\n",
        "\n",
        "def _strip_stress(pron):\n",
        "    return [p[:-1] if p and p[-1].isdigit() else p for p in pron]\n",
        "\n",
        "def _cmu_prons(word, cmu_dict):\n",
        "    w = word.lower()\n",
        "    if w in cmu_dict:\n",
        "        return [_strip_stress(p) for p in cmu_dict[w]]\n",
        "    return []\n",
        "\n",
        "def _phonetic_base_sim(w1, w2, beta=2.0):\n",
        "    pr1 = _cmu_prons(w1, cmu)\n",
        "    pr2 = _cmu_prons(w2, cmu)\n",
        "    best = 0.0\n",
        "    if pr1 and pr2:\n",
        "        for p1 in pr1:\n",
        "            for p2 in pr2:\n",
        "                d = _levenshtein(p1, p2)\n",
        "                s = _weighted_sim_from_distance(d, len(p1), len(p2), beta=beta)\n",
        "                s += _edge_match_bonus(p1, p2)\n",
        "                best = max(best, min(1.0, s))\n",
        "        return best\n",
        "    try:\n",
        "        m1 = phonetics.metaphone(w1)\n",
        "        m2 = phonetics.metaphone(w2)\n",
        "        if m1 and m2:\n",
        "            d = _levenshtein(m1, m2)\n",
        "            s = _weighted_sim_from_distance(d, len(m1), len(m2), beta=beta)\n",
        "            s += _edge_match_bonus(list(m1), list(m2))\n",
        "            return float(min(1.0, s))\n",
        "    except Exception:\n",
        "        pass\n",
        "    s1, s2 = phonetics.soundex(w1), phonetics.soundex(w2)\n",
        "    d = _levenshtein(s1, s2)\n",
        "    s = _weighted_sim_from_distance(d, len(s1), len(s2), beta=beta)\n",
        "    s += _edge_match_bonus(list(s1), list(s2))\n",
        "    return float(min(1.0, s))\n",
        "\n",
        "def phonetic_similarity(word1, word2, penalty_weight=0.0, beta=2.0):\n",
        "    if not word1 or not word2:\n",
        "        return 0.0\n",
        "    prons1 = _cmu_prons(word1, cmu)\n",
        "    prons2 = _cmu_prons(word2, cmu)\n",
        "    if prons1 and prons2:\n",
        "        best = 0.0\n",
        "        for p1 in prons1:\n",
        "            for p2 in prons2:\n",
        "                d = _levenshtein(p1, p2)\n",
        "                s = _weighted_sim_from_distance(d, len(p1), len(p2), beta=beta)\n",
        "                s += _edge_match_bonus(p1, p2)\n",
        "                best = max(best, min(1.0, s))\n",
        "        base_sim = best\n",
        "    else:\n",
        "        try:\n",
        "            m1 = phonetics.metaphone(word1)\n",
        "            m2 = phonetics.metaphone(word2)\n",
        "            if m1 and m2:\n",
        "                d = _levenshtein(m1, m2)\n",
        "                s = _weighted_sim_from_distance(d, len(m1), len(m2), beta=beta)\n",
        "                s += _edge_match_bonus(list(m1), list(m2))\n",
        "                base_sim = float(min(1.0, s))\n",
        "            else:\n",
        "                raise ValueError(\"Empty metaphone\")\n",
        "        except Exception:\n",
        "            s1, s2 = phonetics.soundex(word1), phonetics.soundex(word2)\n",
        "            d = _levenshtein(s1, s2)\n",
        "            s = _weighted_sim_from_distance(d, len(s1), len(s2), beta=beta)\n",
        "            s += _edge_match_bonus(list(s1), list(s2))\n",
        "            base_sim = float(min(1.0, s))\n",
        "    penalty = _letter_penalty(word1, word2) if penalty_weight > 0 else 0.0\n",
        "    final = base_sim - penalty_weight * penalty\n",
        "    return round(float(max(0.0, min(1.0, final))), 4)\n",
        "\n",
        "def _find_subseq(all_ids, sub_ids):\n",
        "    spans = []\n",
        "    n, m = len(all_ids), len(sub_ids)\n",
        "    if m == 0 or n < m:\n",
        "        return spans\n",
        "    for i in range(n - m + 1):\n",
        "        if all_ids[i:i+m] == sub_ids:\n",
        "            spans.append((i, i+m))\n",
        "    return spans\n",
        "\n",
        "def _mean_pool(tensor, dim=0):\n",
        "    return tensor.mean(dim=dim)\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    with torch.no_grad():\n",
        "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
        "        out = bert(**enc)\n",
        "        emb = out.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    n = np.linalg.norm(emb)\n",
        "    return emb / n if n > 0 else emb\n",
        "\n",
        "def get_contextual_embedding(text, target):\n",
        "    with torch.no_grad():\n",
        "        enc_text = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
        "        out_text = bert(**enc_text)\n",
        "        H = out_text.last_hidden_state.squeeze(0)   # [T, D]\n",
        "        ids_text = enc_text[\"input_ids\"].squeeze(0).tolist()\n",
        "\n",
        "        tgt_tokens = tokenizer.tokenize(target)\n",
        "        if not tgt_tokens:\n",
        "            return get_bert_embedding(target)\n",
        "        tgt_ids = tokenizer.convert_tokens_to_ids(tgt_tokens)\n",
        "\n",
        "        spans = _find_subseq(ids_text, tgt_ids)\n",
        "        if spans:\n",
        "            reps = [_mean_pool(H[s:e, :]) for (s, e) in spans]\n",
        "            rep = torch.stack(reps, dim=0).mean(dim=0).cpu().numpy()\n",
        "            n = np.linalg.norm(rep)\n",
        "            return rep / n if n > 0 else rep\n",
        "\n",
        "        return get_bert_embedding(target)\n",
        "\n",
        "def semantic_similarity(text, w1, w2):\n",
        "    e1 = get_contextual_embedding(text, w1)\n",
        "    e2 = get_contextual_embedding(text, w2)\n",
        "    if not np.any(e1) or not np.any(e2):\n",
        "        return 0.0\n",
        "    c = 1 - cosine(e1, e2)\n",
        "    if np.isnan(c):\n",
        "        return 0.0\n",
        "    return float(np.clip(c, -1.0, 1.0))\n",
        "\n",
        "# â”€â”€ implicit phonetic partner indices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "_META_INDEX = None\n",
        "_FIRST_PHONE_INDEX = None\n",
        "\n",
        "def _build_phonetic_indices():\n",
        "    global _META_INDEX, _FIRST_PHONE_INDEX\n",
        "    if _META_INDEX is not None:\n",
        "        return\n",
        "    _META_INDEX = defaultdict(list)\n",
        "    _FIRST_PHONE_INDEX = defaultdict(list)\n",
        "    for w, prons in cmu.items():\n",
        "        try:\n",
        "            mp = phonetics.metaphone(w)\n",
        "            if mp:\n",
        "                _META_INDEX[mp].append(w)\n",
        "            if prons:\n",
        "                first_phone = _strip_stress(prons[0])[0] if prons[0] else None\n",
        "                if first_phone:\n",
        "                    _FIRST_PHONE_INDEX[first_phone].append(w)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "# [UPDATED] Avoid same-lemma candidates in implicit search\n",
        "def _best_implicit_phonetic_partner(word, max_candidates=4000, penalty_weight=0.0, beta=2.0,\n",
        "                                    avoid_same_lemma: bool = True):\n",
        "    _build_phonetic_indices()\n",
        "    word_l = word.lower()\n",
        "    cands = set()\n",
        "    try:\n",
        "        mp = phonetics.metaphone(word_l)\n",
        "        if mp and mp in _META_INDEX:\n",
        "            cands.update(_META_INDEX[mp])\n",
        "    except Exception:\n",
        "        pass\n",
        "    pr = _cmu_prons(word_l, cmu)\n",
        "    if pr:\n",
        "        fp = pr[0][0] if pr[0] else None\n",
        "        if fp and fp in _FIRST_PHONE_INDEX:\n",
        "            cands.update(_FIRST_PHONE_INDEX[fp])\n",
        "    try:\n",
        "        sx = phonetics.soundex(word_l)\n",
        "        target = sx[:1]\n",
        "        if target:\n",
        "            added = 0\n",
        "            for w in cmu.keys():\n",
        "                if w == word_l:\n",
        "                    continue\n",
        "                if phonetics.soundex(w)[:1] == target:\n",
        "                    cands.add(w)\n",
        "                    added += 1\n",
        "                    if added >= max_candidates:\n",
        "                        break\n",
        "    except Exception:\n",
        "        pass\n",
        "    cands.discard(word_l)\n",
        "\n",
        "    # NEW: filter by lemma-equivalence (skip same-base forms like water/watery)\n",
        "    if avoid_same_lemma:\n",
        "        src_lemmas = _lemma_set(word_l)\n",
        "        cands = [c for c in cands if not (_lemma_set(c) & src_lemmas)]\n",
        "\n",
        "    best_w, best_sim = None, 0.0\n",
        "    for cand in cands:\n",
        "        base = _phonetic_base_sim(word_l, cand, beta=beta)\n",
        "        score = base - (penalty_weight * _letter_penalty(word_l, cand) if penalty_weight > 0 else 0.0)\n",
        "        if score > best_sim:\n",
        "            best_sim = score\n",
        "            best_w = cand\n",
        "    if best_w is None:\n",
        "        return None, 0.0\n",
        "    return best_w, round(max(0.0, min(1.0, best_sim)), 4)\n",
        "\n",
        "def aoa_penalty(word, age):\n",
        "    return 0.0\n",
        "\n",
        "def _clean_and_tag(text: str):\n",
        "    text = expand_contractions(text)\n",
        "    raw_tokens = word_tokenize(text)\n",
        "    tokens = [t.lower() for t in raw_tokens if t.isalpha() and len(t) >= 3]\n",
        "    try:\n",
        "        tagged = pos_tag(tokens, lang=\"eng\")\n",
        "    except TypeError:\n",
        "        tagged = pos_tag(tokens)\n",
        "    filtered = [(w, p) for (w, p) in tagged if w not in STOP and w not in AUXILIARIES]\n",
        "    return filtered\n",
        "\n",
        "def extract_features(text, age, penalty_weight=0.0, beta=2.0):\n",
        "    tagged = _clean_and_tag(text)\n",
        "    tagged = [(w, p) for (w, p) in tagged if p.startswith((\"N\", \"V\"))]\n",
        "    seen_lemmas = set()\n",
        "    unique = []\n",
        "    lemma_map = {}\n",
        "    for w, p in tagged:\n",
        "        lem = lemma_token(w, p)\n",
        "        if lem not in seen_lemmas:\n",
        "            seen_lemmas.add(lem)\n",
        "            unique.append(w)\n",
        "            lemma_map[w] = lem\n",
        "\n",
        "    best_pair_phon, best_phon = (\"\", \"\"), 0.0\n",
        "    for i, w1 in enumerate(unique):\n",
        "        for w2 in unique[i+1:]:\n",
        "            if lemma_map.get(w1, w1) == lemma_map.get(w2, w2):\n",
        "                continue\n",
        "            ps = phonetic_similarity(w1, w2, penalty_weight=penalty_weight, beta=beta)\n",
        "            if ps > best_phon:\n",
        "                best_pair_phon, best_phon = (w1, w2), ps\n",
        "\n",
        "    implicit_used = False\n",
        "    if best_phon < 0.75:\n",
        "        implicit_pair, implicit_sim = (\"\", \"\"), 0.0\n",
        "        for w in unique:\n",
        "            iw, isim = _best_implicit_phonetic_partner(\n",
        "                w, penalty_weight=penalty_weight, beta=beta, avoid_same_lemma=True  # â† NEW\n",
        "            )\n",
        "            if iw and isim > implicit_sim:\n",
        "                implicit_pair, implicit_sim = (w, iw), isim\n",
        "        if implicit_sim > best_phon:\n",
        "            best_pair_phon, best_phon = implicit_pair, implicit_sim\n",
        "            implicit_used = True\n",
        "\n",
        "    best_pair_sem, best_sem = (\"\", \"\"), -1.0\n",
        "    for i, w1 in enumerate(unique):\n",
        "        for w2 in unique[i+1:]:\n",
        "            if lemma_map.get(w1, w1) == lemma_map.get(w2, w2):\n",
        "                continue\n",
        "            ss = semantic_similarity(text, w1, w2)   # [-1,1]\n",
        "            if ss > best_sem:\n",
        "                best_pair_sem, best_sem = (w1, w2), ss\n",
        "\n",
        "    head_word = best_pair_phon[0] if best_pair_phon[0] else (unique[0] if unique else \"\")\n",
        "    aoa_diff = aoa_penalty(head_word, age) if head_word else 0.0\n",
        "\n",
        "    if best_pair_phon[0]:\n",
        "        phon_label = f\"({best_pair_phon[0]}, {best_pair_phon[1]})\"\n",
        "        if implicit_used:\n",
        "            phon_label += \" [implicit]\"\n",
        "        phon_label += f\" with similarity: {round(best_phon, 2)}\"\n",
        "    else:\n",
        "        phon_label = \"none\"\n",
        "\n",
        "    return {\n",
        "        \"candidate_phonetic_pairs\": phon_label,\n",
        "        \"candidate_semantic_pairs\": (\n",
        "            f\"({best_pair_sem[0]}, {best_pair_sem[1]}) with similarity: {round(best_sem, 2)}\"\n",
        "            if best_pair_sem[0] else \"none\"\n",
        "        ),\n",
        "        \"phonetic_similarity\": round(best_phon, 2),\n",
        "        \"semantic_similarity\": round(best_sem, 2),\n",
        "        \"aoa_diff\": float(aoa_diff),\n",
        "    }\n",
        "\n",
        "def _safe_take_text(gen_response):\n",
        "    if hasattr(gen_response, \"text\") and gen_response.text:\n",
        "        return gen_response.text\n",
        "    try:\n",
        "        cands = getattr(gen_response, \"candidates\", [])\n",
        "        for c in cands:\n",
        "            content = getattr(c, \"content\", None)\n",
        "            parts = []\n",
        "            if isinstance(content, dict):\n",
        "                parts = content.get(\"parts\", [])\n",
        "            else:\n",
        "                parts = getattr(content, \"parts\", []) if content is not None else []\n",
        "            for p in parts:\n",
        "                t = getattr(p, \"text\", None)\n",
        "                if t:\n",
        "                    return t\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "def llm_reasoning(text, feats, retries=2):\n",
        "    prompt = f\"\"\"\n",
        "Analyze this text for humor and puns.\n",
        "\n",
        "Text: \"{text}\"\n",
        "\n",
        "Features:\n",
        "- Candidate phonetic pair: {feats['candidate_phonetic_pairs']}\n",
        "- Candidate semantic pair: {feats['candidate_semantic_pairs']}\n",
        "- Phonetic similarity: {feats['phonetic_similarity']}\n",
        "- Semantic similarity: {feats['semantic_similarity']}\n",
        "- Age Appropriateness: {feats['aoa_diff']}\n",
        "\n",
        "Tasks:\n",
        "1. Classify the pun type: semantic, phonetic, or non-joke\n",
        "2. Given the answer above if the classification (if the pun type is phonetic, consider only phonetic pair, otherwise selects semantic pair), explain why it might be funny and fit in the context.\n",
        "3. If the text doesn't make a coherent joke with those pairs, set valid_joke=false.\n",
        "4. Determine the appropriate age group for the joke.\n",
        "\n",
        "Respond with ONLY a valid JSON object (no markdown, no code fences):\n",
        "{{\n",
        "  \"pun_type\": \"semantic\",\n",
        "  \"valid_joke\": true,\n",
        "  \"humor_reason\": \"Brief explanation\",\n",
        "  \"age_appropriate\": true\n",
        "}}\n",
        "\n",
        "while printing humor reason consider printing in a new line after every 10 words.\n",
        "\"\"\".strip()\n",
        "\n",
        "    last_err = None\n",
        "    for i in range(retries + 1):\n",
        "        try:\n",
        "            r = model_llm.generate_content(prompt)\n",
        "            response_text = _safe_take_text(r).strip()\n",
        "\n",
        "            response_text = re.sub(r'^\\s*```json\\s*', '', response_text)\n",
        "            response_text = re.sub(r'^\\s*```\\s*', '', response_text)\n",
        "            response_text = re.sub(r'\\s*```$', '', response_text).strip()\n",
        "\n",
        "            m = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "            if m:\n",
        "                response_text = m.group(0)\n",
        "\n",
        "            return json.loads(response_text)\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            msg = str(e)\n",
        "            if any(code in msg for code in (\"429\", \"503\")) and i < retries:\n",
        "                time.sleep(2 * (i + 1))\n",
        "                continue\n",
        "            break\n",
        "\n",
        "    print(f\"âŒ LLM error: {last_err}\")\n",
        "    return {\n",
        "        \"pun_type\": \"none\",\n",
        "        \"valid_joke\": False,\n",
        "        \"humor_reason\": f\"Error: {str(last_err)[:200]}\",\n",
        "        \"age_appropriate\": False\n",
        "    }\n",
        "\n",
        "def analyze_text(text, age):\n",
        "    feats = extract_features(text, age)\n",
        "    verdict = llm_reasoning(text, feats)\n",
        "    return {**feats, **verdict}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLSA_rKWwVnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1afa4ac-4c73-4834-d1a5-59906c0e4fcc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversational Agent (memory on) â€” type 'quit' to exit.\n",
            "Agent: It is a pun (phonetic). Selected phonetic pair: 'debris'â€“'brie'\n",
            "(channel: sound). The phrase 'the debris' sounds like 'de brie',\n",
            "which is a type of French cheese mentioned in the\n",
            "setup. The humor is a phonetic pun on the word\n",
            "'debris'. It sounds like 'de Brie', a type of French\n",
            "cheese. The joke creates a funny image of both wreckage\n",
            "and cheese scattered everywhere.\n",
            "Agent: phonetic â€” The phrase 'the debris' sounds like 'de\n",
            "brie', which is a type of French cheese mentioned in\n",
            "the setup.\n",
            "Agent: Chosen phonetic pair 'debris'â€“'brie' has phonetic similarity 0.91.\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Conversational Agent with MEMORY â€” LLM-SELECTED CANDIDATE PAIR\n",
        "# Behavior change:\n",
        "#  â€¢ Do NOT print similarity scores in normal answers.\n",
        "#    Only reveal similarity when explicitly asked.\n",
        "#  â€¢ Output is wrapped to â‰¤10 words/line for easy snipping.\n",
        "# Requirements (already defined earlier in your notebook):\n",
        "#   - analyze_text(text, age), extract_features(text, age)\n",
        "#   - phonetic_similarity(w1, w2), semantic_similarity(text, w1, w2)\n",
        "#   - model_llm (Gemini) configured\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "import json, re, time\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "\n",
        "# ---------- Word-wrap helper (â‰¤10 words per line) ----------\n",
        "def _wrap_by_words(text: str, max_words: int = 10) -> str:\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return \"\"\n",
        "    lines, cur = [], []\n",
        "    for w in words:\n",
        "        cur.append(w)\n",
        "        if len(cur) >= max_words:\n",
        "            lines.append(\" \".join(cur))\n",
        "            cur = []\n",
        "    if cur:\n",
        "        lines.append(\" \".join(cur))\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# --- small utility (define if not in scope) ---\n",
        "def _safe_take_text(gen_response):\n",
        "    if hasattr(gen_response, \"text\") and gen_response.text:\n",
        "        return gen_response.text\n",
        "    try:\n",
        "        cands = getattr(gen_response, \"candidates\", [])\n",
        "        for c in cands:\n",
        "            content = getattr(c, \"content\", None)\n",
        "            parts = content.get(\"parts\", []) if isinstance(content, dict) else getattr(content, \"parts\", [])\n",
        "            for p in parts or []:\n",
        "                t = getattr(p, \"text\", None)\n",
        "                if t: return t\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "def _parse_pair_label(lbl: str) -> Tuple[Optional[str], Optional[str], bool, Optional[float]]:\n",
        "    if not lbl or lbl.strip().lower() == \"none\":\n",
        "        return None, None, False, None\n",
        "    implicit = \"[implicit]\" in lbl\n",
        "    m = re.search(r\"\\(([^,]+),\\s*([^)]+)\\)\", lbl)\n",
        "    w1, w2 = (m.group(1).strip(), m.group(2).strip()) if m else (None, None)\n",
        "    ms = re.search(r\"similarity:\\s*([0-9.]+)\", lbl)\n",
        "    score = float(ms.group(1)) if ms else None\n",
        "    return w1, w2, implicit, score\n",
        "\n",
        "# â”€â”€ LLM prompt for choosing the pair (ignore upstream pairs; pick your own) â”€â”€â”€\n",
        "_CHOOSE_PAIR_INSTRUCTIONS = \"\"\"\n",
        "You are assisting a pun/wordplay detector. You will receive:\n",
        "- The input sentence (text) and age\n",
        "- The detector's coarse verdict about pun type: one of {\"phonetic\",\"semantic\",\"non-joke\"}.\n",
        "\n",
        "Your task:\n",
        "1) Choose EXACTLY ONE candidate pair using your reasoning (do not rely on earlier candidate strings).\n",
        "   They must be two different words.\n",
        "   â€¢ If pun_type == \"phonetic\": you may pick an implicit phonetic partner; it need not both appear in text.\n",
        "   â€¢ If pun_type == \"semantic\": pick two different words that appear in the given text and are meaning-related for the joke.\n",
        "2) If pun_type == \"non-joke\", set chosen_type=\"none\" and return empty pair.\n",
        "3) Return STRICT JSON (no markdown, no extra text):\n",
        "{\n",
        "  \"chosen_type\": \"phonetic\" | \"semantic\" | \"none\",\n",
        "  \"w1\": \"<first word or empty>\",\n",
        "  \"w2\": \"<second word or empty>\",\n",
        "  \"why\": \"one short sentence\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def llm_choose_pair(text: str, age: float, pun_type_hint: str, retries: int = 1) -> Dict[str, Any]:\n",
        "    payload = {\"text\": text, \"age\": age, \"pun_type_hint\": pun_type_hint}\n",
        "    prompt = (\n",
        "        _CHOOSE_PAIR_INSTRUCTIONS\n",
        "        + \"\\n\\nINPUT:\\n\"\n",
        "        + json.dumps(payload, ensure_ascii=False, indent=2)\n",
        "        + \"\\n\\nOUTPUT JSON ONLY:\"\n",
        "    )\n",
        "    last_err = None\n",
        "    for i in range(retries + 1):\n",
        "        try:\n",
        "            r = model_llm.generate_content(prompt)\n",
        "            ans = _safe_take_text(r).strip()\n",
        "            ans = re.sub(r'^\\s*```(?:json)?\\s*', '', ans)\n",
        "            ans = re.sub(r'\\s*```\\s*$', '', ans).strip()\n",
        "            m = re.search(r'\\{.*\\}', ans, re.DOTALL)\n",
        "            if m: ans = m.group(0)\n",
        "            data = json.loads(ans)\n",
        "            ctype = str(data.get(\"chosen_type\", \"none\")).lower()\n",
        "            if ctype not in {\"phonetic\", \"semantic\", \"none\"}:\n",
        "                ctype = \"none\"\n",
        "            w1 = (data.get(\"w1\") or \"\").strip()\n",
        "            w2 = (data.get(\"w2\") or \"\").strip()\n",
        "            why = (data.get(\"why\") or \"\").strip()\n",
        "            if w1 and w2 and w1.lower() == w2.lower():\n",
        "                ctype, w1, w2 = \"none\", \"\", \"\"\n",
        "            return {\"chosen_type\": ctype, \"w1\": w1, \"w2\": w2, \"why\": why}\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if i < retries and any(x in str(e) for x in (\"429\", \"503\")):\n",
        "                time.sleep(1.5 * (i + 1)); continue\n",
        "            break\n",
        "    return {\"chosen_type\": \"none\", \"w1\": \"\", \"w2\": \"\", \"why\": f\"[LLM choose error] {str(last_err)[:160]}\"}\n",
        "\n",
        "# â”€â”€ LLM QA constrained to SELECTED pair (for follow-ups & uncommon Qs) â”€â”€â”€â”€â”€â”€â”€\n",
        "_QA_SELECTED_PAIR_INSTRUCTIONS = \"\"\"\n",
        "You answer questions about the detected wordplay using ONLY the SELECTED pair and its channel.\n",
        "Do not invent new pairs. Be concise and plain text.\n",
        "If asked â€œwhat kind of wordplay,â€ answer with {phonetic|semantic|none} and one short reason.\n",
        "Do not report any similarity score unless the user explicitly asks for it.\n",
        "\"\"\"\n",
        "\n",
        "def qa_on_selected(question: str, state, retries: int = 1) -> str:\n",
        "    sel = state.selected or {}\n",
        "    blob = {\n",
        "        \"text\": state.last_text or \"\",\n",
        "        \"age\": state.last_age,\n",
        "        \"selected_type\": sel.get(\"chosen_type\",\"none\"),\n",
        "        \"selected_pair\": [sel.get(\"w1\",\"\"), sel.get(\"w2\",\"\")],\n",
        "        \"notes\": \"Use only the selected pair carried from prior analysis.\"\n",
        "    }\n",
        "    prompt = (\n",
        "        _QA_SELECTED_PAIR_INSTRUCTIONS\n",
        "        + \"\\n\\nQUESTION:\\n\" + question.strip()\n",
        "        + \"\\n\\nCONTEXT:\\n\" + json.dumps(blob, ensure_ascii=False, indent=2)\n",
        "        + \"\\n\\nANSWER (plain text, concise):\"\n",
        "    )\n",
        "    last_err = None\n",
        "    for i in range(retries + 1):\n",
        "        try:\n",
        "            r = model_llm.generate_content(prompt)\n",
        "            ans = _safe_take_text(r).strip()\n",
        "            ans = re.sub(r'^\\s*```(?:json)?\\s*', '', ans)\n",
        "            ans = re.sub(r'\\s*```\\s*$', '', ans).strip()\n",
        "            return ans\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if i < retries and any(x in str(e) for x in (\"429\", \"503\")):\n",
        "                time.sleep(1.5 * (i + 1)); continue\n",
        "            break\n",
        "    return f\"[LLM error] {str(last_err)[:200]}\"\n",
        "\n",
        "# â”€â”€ Conversation memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class ConversationState:\n",
        "    def __init__(self, default_age: float = 10):\n",
        "        self.last_text: Optional[str] = None\n",
        "        self.last_age: float = float(default_age)\n",
        "        self.last_feats: Optional[Dict[str,Any]] = None\n",
        "        self.selected: Dict[str, Any] = {\n",
        "            \"chosen_type\": \"none\",\n",
        "            \"w1\": \"\",\n",
        "            \"w2\": \"\",\n",
        "            \"why\": \"\",\n",
        "            \"similarity\": None  # kept for follow-ups, not printed by default\n",
        "        }\n",
        "\n",
        "    def update_selection(self, chosen_type: str, w1: str, w2: str, why: str, similarity: Optional[float]):\n",
        "        self.selected = {\"chosen_type\": chosen_type, \"w1\": w1, \"w2\": w2, \"why\": why, \"similarity\": similarity}\n",
        "\n",
        "    def update_sentence(self, text: str, age: float, feats: Dict[str, Any]):\n",
        "        self.last_text = text\n",
        "        self.last_age = float(age)\n",
        "        self.last_feats = feats\n",
        "\n",
        "STATE = ConversationState(default_age=10)\n",
        "\n",
        "# â”€â”€ Age/sentence extraction helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "_AGE_PAT = re.compile(r\"age\\s*=\\s*(\\d{1,3})\", re.I)\n",
        "\n",
        "def _extract_age(user: str, fallback: float) -> float:\n",
        "    m = _AGE_PAT.search(user)\n",
        "    if m:\n",
        "        try: return float(m.group(1))\n",
        "        except: pass\n",
        "    return float(fallback)\n",
        "\n",
        "def _extract_sentence(user: str, last_text: Optional[str]) -> Optional[str]:\n",
        "    qm = re.findall(r\"â€œ([^â€]+)â€|\\\"([^\\\"]+)\\\"\", user)\n",
        "    for grp in qm:\n",
        "        inside = grp[0] or grp[1]\n",
        "        if inside and len(inside.split()) >= 2:\n",
        "            return inside.strip()\n",
        "    m = re.search(r\"\\bis\\s+(.+?)\\s+a\\s+pun\\??\", user, flags=re.I)\n",
        "    if m:\n",
        "        cand = m.group(1).strip()\n",
        "        if len(cand.split()) >= 2: return cand\n",
        "    m = re.search(r\"(?:^|\\b)text\\s*:\\s*(.+)$\", user, flags=re.I)\n",
        "    if m: return m.group(1).strip()\n",
        "    if len(user.split()) >= 3 and any(p in user for p in [\".\", \"?\", \"!\", \"â€”\", \",\"]):\n",
        "        return user.strip()\n",
        "    return last_text\n",
        "\n",
        "# â”€â”€ Follow-up patterns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "_SIM_PHON_PAT = re.compile(r\"phonetic\\s+similarity\\s+between\\s+([a-z]+)\\s+and\\s+([a-z]+)\", re.I)\n",
        "_SIM_SEM_PAT  = re.compile(r\"semantic\\s+(?:similarity|distance)\\s+between\\s+([a-z]+)\\s+and\\s+([a-z]+)\", re.I)\n",
        "\n",
        "# â”€â”€ Core analyze+select pipeline (LLM chooses the pair) â”€â”€â”€â”€â”€â”€â”€\n",
        "def _analyze_and_select(text: str, age: float) -> str:\n",
        "    feats = extract_features(text, age)\n",
        "    verdict = analyze_text(text, age)\n",
        "    pun_type = str(verdict.get(\"pun_type\", \"none\")).lower()\n",
        "    if pun_type == \"non-joke\":\n",
        "        pun_type = \"none\"\n",
        "\n",
        "    choice = llm_choose_pair(text, age, pun_type)\n",
        "    chosen_type = choice[\"chosen_type\"]\n",
        "    w1, w2 = choice[\"w1\"], choice[\"w2\"]\n",
        "\n",
        "    # compute but don't print similarity unless asked later\n",
        "    similarity = None\n",
        "    if chosen_type == \"phonetic\" and w1 and w2:\n",
        "        similarity = float(phonetic_similarity(w1, w2))\n",
        "    elif chosen_type == \"semantic\" and w1 and w2:\n",
        "        similarity = float(semantic_similarity(text, w1, w2))\n",
        "\n",
        "    STATE.update_sentence(text, age, feats)\n",
        "    STATE.update_selection(chosen_type, w1, w2, choice.get(\"why\",\"\"), similarity)\n",
        "\n",
        "    is_joke = verdict.get(\"valid_joke\", False)\n",
        "    human = []\n",
        "    human.append(f\"It is {'a pun' if is_joke else 'not a pun'} ({pun_type}).\")\n",
        "    if chosen_type in {\"phonetic\",\"semantic\"} and w1 and w2:\n",
        "        chan = \"sound\" if chosen_type == \"phonetic\" else \"meaning\"\n",
        "        human.append(f\"Selected {chosen_type} pair: '{w1}'â€“'{w2}' (channel: {chan}).\")\n",
        "    if choice.get(\"why\"):\n",
        "        human.append(choice[\"why\"])\n",
        "    if verdict.get(\"humor_reason\"):\n",
        "        human.append(verdict[\"humor_reason\"])\n",
        "    return \" \".join(human).strip()\n",
        "\n",
        "# â”€â”€ Router (intent checks BEFORE generic analyze) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def dispatch(user_query: str, state: ConversationState) -> str:\n",
        "    q = user_query.strip()\n",
        "    ql = q.lower()\n",
        "    age = _extract_age(q, state.last_age)\n",
        "    text = _extract_sentence(q, state.last_text)\n",
        "\n",
        "    # direct similarity queries (explicit â‡’ allowed to print scores)\n",
        "    m = _SIM_PHON_PAT.search(ql)\n",
        "    if m:\n",
        "        w1, w2 = m.group(1), m.group(2)\n",
        "        return f\"Phonetic similarity between '{w1}' and '{w2}' is {phonetic_similarity(w1, w2):.2f}.\"\n",
        "    m = _SIM_SEM_PAT.search(ql)\n",
        "    if m:\n",
        "        w1, w2 = m.group(1), m.group(2)\n",
        "        ctx_text = text or state.last_text or f\"{w1} â€¦ {w2}.\"\n",
        "        sim = semantic_similarity(ctx_text, w1, w2)\n",
        "        return f\"Semantic similarity between '{w1}' and '{w2}' is {sim:.2f} (distance {1-sim:.2f}).\"\n",
        "\n",
        "    # follow-ups on the SELECTED pair (explicit similarity â‡’ allowed)\n",
        "    if \"semantic distance between the chosen pair\" in ql or \"semantic similarity of the chosen pair\" in ql:\n",
        "        sel = state.selected\n",
        "        if not sel or sel.get(\"chosen_type\") != \"semantic\" or not (sel.get(\"w1\") and sel.get(\"w2\")):\n",
        "            return \"No semantic pair is selected as it has classified as phonetic.\"\n",
        "        sim = semantic_similarity(state.last_text, sel[\"w1\"], sel[\"w2\"])\n",
        "        state.selected[\"similarity\"] = float(sim)\n",
        "        return f\"Chosen semantic pair '{sel['w1']}'â€“'{sel['w2']}' has similarity {sim:.2f} (distance {1-sim:.2f}).\"\n",
        "\n",
        "    if \"phonetic similarity of the chosen pair\" in ql or \"sound similarity of the chosen pair\" in ql:\n",
        "        sel = state.selected\n",
        "        if not sel or sel.get(\"chosen_type\") != \"phonetic\" or not (sel.get(\"w1\") and sel.get(\"w2\")):\n",
        "            return \"No phonetic pair is selected as the jokes classified as phonetic.\"\n",
        "        sim = phonetic_similarity(sel[\"w1\"], sel[\"w2\"])\n",
        "        state.selected[\"similarity\"] = float(sim)\n",
        "        return f\"Chosen phonetic pair '{sel['w1']}'â€“'{sel['w2']}' has phonetic similarity {sim:.2f}.\"\n",
        "\n",
        "    # â€œwhat kind of wordplay / pun type / type of punâ€ â€” use selected pair (no scores)\n",
        "    if (\"what kind of wordplay\" in ql) or re.search(r\"\\bpun\\s*type\\b\", ql) or re.search(r\"\\btype\\s+of\\s+pun\\b\", ql):\n",
        "        sel = state.selected\n",
        "        if not state.last_text or not sel or sel.get(\"chosen_type\") == \"none\":\n",
        "            return \"Provide a sentence first so I can select a pair (e.g., Is â€œWater you doing for lunchâ€ a pun? age=10).\"\n",
        "        pt = sel.get(\"chosen_type\", \"none\")\n",
        "        if pt in {\"phonetic\",\"semantic\"}:\n",
        "            reason = sel.get(\"why\",\"The selected pair best matches the detected wordplay channel.\")\n",
        "            return f\"{pt} â€” {reason}\"\n",
        "        return \"none â€” no convincing pair was selected for wordplay.\"\n",
        "\n",
        "    # â€œexplain the joke/punâ€ â€” use selected pair (no scores)\n",
        "    if \"explain\" in ql and (\"joke\" in ql or \"pun\" in ql):\n",
        "        sel = state.selected\n",
        "        if not state.last_text or not sel or sel.get(\"chosen_type\") == \"none\":\n",
        "            return \"Provide a sentence first so I can select a pair.\"\n",
        "        return qa_on_selected(q, state)\n",
        "\n",
        "    # â€œIs â€¦ a pun?â€ or sentence present â†’ analyze+select (no scores in summary)\n",
        "    if re.search(r\"\\bis\\s+.+\\bpun\\??\", ql) or (text and len(text.split()) >= 2):\n",
        "        return _analyze_and_select(text, age) if text else \"Please include a sentence.\"\n",
        "\n",
        "    # Generic uncommon questions â†’ LLM QA using selected pair (no scores)\n",
        "    if state.last_text and state.selected and state.selected.get(\"chosen_type\") != \"none\":\n",
        "        return qa_on_selected(q, state)\n",
        "\n",
        "    return \"Tell me what to do (e.g., Is â€œWater you doing for lunchâ€ a pun? age=10).\"\n",
        "\n",
        "# â”€â”€ One-box loop with memory + wrapped output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"Conversational Agent (memory on) â€” type 'quit' to exit.\")\n",
        "#print(\"Examples:\")\n",
        "#print('  â€¢ Is \"Water you doing for lunch\" a pun? age=10')\n",
        "#print('  â€¢ phonetic similarity between x and y')\n",
        "#print('  â€¢ semantic distance between the chosen pair')\n",
        "#print('  â€¢ semantic similarity of the chosen pair')\n",
        "#print('  â€¢ phonetic similarity of the chosen pair')\n",
        "#print('  â€¢ what kind of wordplay is involved?')\n",
        "#print('  â€¢ explain the joke')\n",
        "\n",
        "\n",
        "while True:\n",
        "    user = input(\"\\nAsk: \").strip()\n",
        "    if user.lower() in {\"quit\", \"exit\"}:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    raw = dispatch(user, STATE)\n",
        "    print(_wrap_by_words(f\"Agent: {raw}\", max_words=10))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}